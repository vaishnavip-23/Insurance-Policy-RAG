import streamlit as st
import sys
from pathlib import Path

# Add project src/ to path dynamically (works in local and container runs)
ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT / "src"))

from retrieval import hybrid_retrieval, merge_and_rerank
from answer_gen import generate_answer

# Page config
st.set_page_config(
    page_title="Insurance Policy RAG",
    page_icon="ğŸ“„",
    layout="wide"
)

# Title
st.title("ğŸ“„ Insurance Policy RAG Assistant")
st.markdown("Ask questions about your insurance policy and get accurate answers with citations.")

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "citations" in message:
            with st.expander("ğŸ“š View Citations"):
                for i, citation in enumerate(message["citations"], 1):
                    st.markdown(f"**{i}.** Chunk {citation['chunk_id']} (Pages {citation['page_start']}-{citation['page_end']})")
            if "confidence" in message:
                confidence_color = {
                    "high": "ğŸŸ¢",
                    "medium": "ğŸŸ¡",
                    "low": "ğŸ”´"
                }
                st.caption(f"{confidence_color.get(message['confidence'], 'âšª')} Confidence: {message['confidence']}")

# Chat input
if query := st.chat_input("Ask a question about your insurance policy..."):
    # Add user message to chat
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)
    
    # Generate response
    with st.chat_message("assistant"):
        try:
            with st.status("Running pipeline...", expanded=True) as status:
                status.write("Translating query and running dense + BM25 retrieval...")
                dense_results, sparse_results = hybrid_retrieval(query)
                
                status.write("Merging and reranking results (RRF)...")
                final_results = merge_and_rerank(dense_results, sparse_results, top_k=10)
                
                status.write("Generating answer with inline citations...")
                answer = generate_answer(query, final_results)
                status.update(label="Done", state="complete")
            
            # Display answer (includes inline citations)
            st.markdown(answer.answer)
            
            # Add to chat history
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer.answer,
                "citations": [
                    {
                        "chunk_id": c.chunk_id,
                        "page_start": c.page_start,
                        "page_end": c.page_end
                    } for c in answer.citations
                ],
                "confidence": answer.confidence
            })
            
            # Display citations
            with st.expander("ğŸ“š View Citations"):
                for i, citation in enumerate(answer.citations, 1):
                    st.markdown(f"**{i}.** Chunk {citation.chunk_id} (Pages {citation.page_start}-{citation.page_end})")
            
            # Display retrieval stats and confidence in sidebar
            with st.sidebar:
                st.subheader("ğŸ“Š Retrieval Stats")
                st.metric("Chunks Retrieved", final_results.total_before_dedup)
                st.metric("Unique Chunks", final_results.total_after_dedup)
                st.metric("Top Chunks Used", len(final_results.chunks))
                st.caption(f"Confidence: {answer.confidence}")
                
                with st.expander("ğŸ” View Retrieved Chunks"):
                    for i, chunk in enumerate(final_results.chunks, 1):
                        sources_str = " + ".join(chunk.sources)
                        st.markdown(f"**{i}. Chunk {chunk.chunk_id}**")
                        st.caption(f"RRF Score: {chunk.rrf_score} | Sources: {sources_str} | Pages: {chunk.page_start}-{chunk.page_end}")
                        st.text(chunk.text[:200] + "...")
                        st.divider()
        
        except Exception as e:
            st.error(f"âŒ Error: {str(e)}")
            st.exception(e)

# Sidebar
with st.sidebar:
    st.header("â„¹ï¸ About")
    st.markdown("""
    This RAG system uses:
    - **Query Translation**: Generates multiple query variations
    - **Dense Retrieval**: Semantic search on chunk summaries
    - **Sparse Retrieval**: BM25 keyword search on full text
    - **RRF Reranking**: Reciprocal Rank Fusion for merging results
    - **LLM Answer Generation**: GPT-5-mini with citations
    """)
    
    if st.button("ğŸ—‘ï¸ Clear Chat History"):
        st.session_state.messages = []
        st.rerun()
